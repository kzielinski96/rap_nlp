{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "durable-arrival",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amber-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/tf/main_lines_verses.csv\", encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powerful-wichita",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52184</th>\n",
       "      <td>Eminem</td>\n",
       "      <td>Though what you sacrifice barely is half, neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146363</th>\n",
       "      <td>Gunna</td>\n",
       "      <td>They flexed on me with rolls, shit don't compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107826</th>\n",
       "      <td>Logic</td>\n",
       "      <td>I got four cards and they all black, got four ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157376</th>\n",
       "      <td>Napalm</td>\n",
       "      <td>She was pissed and I ended up seein' my physician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97785</th>\n",
       "      <td>Kendrick Lamar</td>\n",
       "      <td>Taylor made a career out of music from writing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           artist_name                                               line\n",
       "52184           Eminem  Though what you sacrifice barely is half, neve...\n",
       "146363           Gunna  They flexed on me with rolls, shit don't compa...\n",
       "107826           Logic  I got four cards and they all black, got four ...\n",
       "157376          Napalm  She was pissed and I ended up seein' my physician\n",
       "97785   Kendrick Lamar  Taylor made a career out of music from writing..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "several-eleven",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 2Pac & Tyrone Wrice     5\n",
       " Drake                  22\n",
       " Goldie Loc             79\n",
       " King Gordy             32\n",
       " Method Man             17\n",
       "                        ..\n",
       "iNTeLL                  16\n",
       "mark curry              18\n",
       "p. diddy over Hook       8\n",
       "stic.man, dead prez      9\n",
       "will.i.am               13\n",
       "Name: artist_name, Length: 1262, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist_name'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proud-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.replace(\"Jay-Z\", \"JAY-Z\", inplace=True)\n",
    "# df.replace(\"KRS-One\", \"KRS-ONE\", inplace=True)\n",
    "# df.replace(\"LL Cool J\", \"L.L. Cool J\", inplace=True)\n",
    "# df.replace(\"Royce da 5'9\", \"Royce Da 5'9\", inplace=True)\n",
    "df.replace(\"Notorious B.I.G.\", \"The Notorious B.I.G.\", inplace=True)\n",
    "df = df.groupby('artist_name').filter(lambda x : len(x)>1000)\n",
    "df = df[df['line'].apply(lambda x: len(x.split(\" \")) > 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['The Notorious B.I.G.', 'Ice Cube', 'Nas', '2Pac', 'Method Man', 'Eminem',\n",
    "        'Snoop Dogg', 'DMX', 'Dr. Dre', 'GZA', 'The Notorious B.I.G.', 'RZA']\n",
    "df = df[df['artist_name'].isin(list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extensive-checkout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eminem                  8377\n",
       "Ice Cube                4381\n",
       "Nas                     3821\n",
       "2Pac                    3775\n",
       "Method Man              3331\n",
       "The Notorious B.I.G.    2695\n",
       "Snoop Dogg              2626\n",
       "DMX                     2562\n",
       "Dr. Dre                 2235\n",
       "GZA                     2033\n",
       "RZA                     1762\n",
       "Name: artist_name, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becoming-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_features = df['line']\n",
    "lyrics_labels = df.pop('artist_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chemical-wrapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52184         Eminem\n",
       "70146     Method Man\n",
       "76335       Ice Cube\n",
       "34093            DMX\n",
       "81164         Eminem\n",
       "             ...    \n",
       "24910         Eminem\n",
       "193116    Method Man\n",
       "47997         Eminem\n",
       "137243    Method Man\n",
       "49209         Eminem\n",
       "Name: artist_name, Length: 37598, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "renewable-following",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2Pac',\n",
       " 'DMX',\n",
       " 'Dr. Dre',\n",
       " 'Eminem',\n",
       " 'GZA',\n",
       " 'Ice Cube',\n",
       " 'Method Man',\n",
       " 'Nas',\n",
       " 'RZA',\n",
       " 'Snoop Dogg',\n",
       " 'The Notorious B.I.G.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(lyrics_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "honey-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(lyrics_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "considered-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_labels = lyrics_labels.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "optimum-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '2Pac',\n",
       " 1: 'DMX',\n",
       " 2: 'Dr. Dre',\n",
       " 3: 'Eminem',\n",
       " 4: 'GZA',\n",
       " 5: 'Ice Cube',\n",
       " 6: 'Method Man',\n",
       " 7: 'Nas',\n",
       " 8: 'RZA',\n",
       " 9: 'Snoop Dogg',\n",
       " 10: 'The Notorious B.I.G.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_ids = dict(enumerate(lyrics_labels.cat.categories))\n",
    "artist_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "filled-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_labels = lyrics_labels.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adult-counter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52184     3\n",
       "70146     6\n",
       "76335     5\n",
       "34093     1\n",
       "81164     3\n",
       "         ..\n",
       "24910     3\n",
       "193116    6\n",
       "47997     3\n",
       "137243    6\n",
       "49209     3\n",
       "Length: 37598, dtype: int8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hazardous-radio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(lyrics_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "concerned-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_ds = tf.data.Dataset.from_tensor_slices((lyrics_features, lyrics_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "convenient-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 38000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "circular-colorado",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:  b'You are your car, what could represent me?'\n",
      "Artist: 7\n",
      "Line:  b\"Yours is shabby and scab, while mine's glistenin'\"\n",
      "Artist: 7\n",
      "Line:  b\"Some niggas'll get money and pay niggas to back em'\"\n",
      "Artist: 2\n",
      "Line:  b'Over girls like you with the BIG ol butts'\n",
      "Artist: 5\n",
      "Line:  b'Have the biggest dick, but when your shell get hit'\n",
      "Artist: 10\n",
      "Line:  b\"Slow music, H-Town, no that's down low\"\n",
      "Artist: 7\n",
      "Line:  b'All I did was give you a style for you to run with'\n",
      "Artist: 7\n",
      "Line:  b\"They want heat, I'll give it to them burnt and crispy\"\n",
      "Artist: 4\n",
      "Line:  b\"Livin' underage, but he'll blaze on your bitch-ass\"\n",
      "Artist: 0\n",
      "Line:  b'And up in yo bitch, is where ya might find me'\n",
      "Artist: 9\n"
     ]
    }
   ],
   "source": [
    "all_labeled_data = lyrics_ds.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "\n",
    "for text, label in all_labeled_data.take(10):\n",
    "  print(\"Line: \", text.numpy())\n",
    "  print(\"Artist:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dried-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-payroll",
   "metadata": {},
   "source": [
    "### PROCESSING DATA FOR CNN CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sound-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fiscal-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, unused_label):\n",
    "  lower_case = tf_text.case_fold_utf8(text)\n",
    "  return tokenizer.tokenize(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dedicated-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\korni\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = all_labeled_data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "casual-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  [b'plenty' b'a' b'times' b'a' b'nigga' b'slipped' b'and' b'fell']\n",
      "Tokens:  [b'talk' b'to' b'you' b'for' b'a' b'minute' b'then' b'my' b'dick' b\"'\"\n",
      " b's' b'in' b'you']\n",
      "Tokens:  [b'and' b'that' b\"'\" b's' b'how' b'you' b'get' b'it' b',20' b'years' b'in'\n",
      " b'a' b'row']\n",
      "Tokens:  [b'anybody' b'in' b'your' b'crew' b'when' b'i']\n",
      "Tokens:  [b'he' b'inhaled' b'so' b'deep' b',' b'shut' b'his' b'eyes' b'like' b'he'\n",
      " b'was' b'sleep']\n"
     ]
    }
   ],
   "source": [
    "for text_batch in tokenized_ds.take(5):\n",
    "  print(\"Tokens: \", text_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "black-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "executive-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10000\n",
      "First five vocab entries: [b\"'\", b',', b'the', b'i', b'a']\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = configure_dataset(tokenized_ds)\n",
    "\n",
    "vocab_dict = collections.defaultdict(lambda: 0)\n",
    "for toks in tokenized_ds.as_numpy_iterator():\n",
    "  for tok in toks:\n",
    "    vocab_dict[tok] += 1\n",
    "\n",
    "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab = [token for token, count in vocab]\n",
    "vocab = vocab[:VOCAB_SIZE]\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "print(\"First five vocab entries:\", vocab[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bulgarian-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = vocab\n",
    "values = range(2, len(vocab) + 2)  # reserve 0 for padding, 1 for OOV\n",
    "\n",
    "init = tf.lookup.KeyValueTensorInitializer(\n",
    "    keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "\n",
    "num_oov_buckets = 1\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "characteristic-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, label):\n",
    "  standardized = tf_text.case_fold_utf8(text)\n",
    "  tokenized = tokenizer.tokenize(standardized)\n",
    "  vectorized = vocab_table.lookup(tokenized)\n",
    "  return vectorized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "champion-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  b'Plenty a times a nigga slipped and fell'\n",
      "Vectorized sentence:  [1570    6  417    6   50 2093    9  773]\n"
     ]
    }
   ],
   "source": [
    "example_text, example_label = next(iter(all_labeled_data))\n",
    "print(\"Sentence: \", example_text.numpy())\n",
    "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
    "print(\"Vectorized sentence: \", vectorized_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mounted-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_data = all_labeled_data.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "lovely-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
    "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "voluntary-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.padded_batch(BATCH_SIZE)\n",
    "validation_data = validation_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "incorporate-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text batch shape:  (64, 19)\n",
      "Label batch shape:  (64,)\n",
      "First text example:  tf.Tensor(\n",
      "[1570    6  417    6   50 2093    9  773    0    0    0    0    0    0\n",
      "    0    0    0    0    0], shape=(19,), dtype=int64)\n",
      "First label example:  tf.Tensor(1, shape=(), dtype=int8)\n"
     ]
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(validation_data))\n",
    "print(\"Text batch shape: \", sample_text.shape)\n",
    "print(\"Label batch shape: \", sample_labels.shape)\n",
    "print(\"First text example: \", sample_text[0])\n",
    "print(\"First label example: \", sample_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "compound-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "blocked-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = configure_dataset(train_data)\n",
    "validation_data = configure_dataset(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-fiber",
   "metadata": {},
   "source": [
    "## TRAIN MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-share",
   "metadata": {},
   "source": [
    "### CLASSIFIER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "racial-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "        layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(num_labels)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "behind-fellow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "541/541 [==============================] - 52s 33ms/step - loss: 2.2648 - accuracy: 0.2260 - val_loss: 2.0192 - val_accuracy: 0.3087\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 1.8986 - accuracy: 0.3679 - val_loss: 1.8757 - val_accuracy: 0.3720\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 1.5203 - accuracy: 0.5114 - val_loss: 1.8685 - val_accuracy: 0.3977\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 1.1850 - accuracy: 0.6291 - val_loss: 1.9533 - val_accuracy: 0.3983\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 0.9027 - accuracy: 0.7252 - val_loss: 2.1017 - val_accuracy: 0.3957\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.6834 - accuracy: 0.8016 - val_loss: 2.2861 - val_accuracy: 0.3887\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 0.5180 - accuracy: 0.8578 - val_loss: 2.4933 - val_accuracy: 0.3893\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 0.3948 - accuracy: 0.8972 - val_loss: 2.7082 - val_accuracy: 0.3863\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 0.3025 - accuracy: 0.9241 - val_loss: 2.9361 - val_accuracy: 0.3797\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 12s 21ms/step - loss: 0.2328 - accuracy: 0.9443 - val_loss: 3.1694 - val_accuracy: 0.3727\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 0.1793 - accuracy: 0.9592 - val_loss: 3.4080 - val_accuracy: 0.3700\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 11s 20ms/step - loss: 0.1372 - accuracy: 0.9713 - val_loss: 3.6534 - val_accuracy: 0.3697\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 0.1042 - accuracy: 0.9800 - val_loss: 3.9016 - val_accuracy: 0.3680\n",
      "Epoch 14/20\n",
      "541/541 [==============================] - 12s 21ms/step - loss: 0.0783 - accuracy: 0.9870 - val_loss: 4.1521 - val_accuracy: 0.3653\n",
      "Epoch 15/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.0593 - accuracy: 0.9909 - val_loss: 4.3939 - val_accuracy: 0.3677\n",
      "Epoch 16/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.0455 - accuracy: 0.9937 - val_loss: 4.6318 - val_accuracy: 0.3640\n",
      "Epoch 17/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.0362 - accuracy: 0.9953 - val_loss: 4.8626 - val_accuracy: 0.3663\n",
      "Epoch 18/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.0301 - accuracy: 0.9963 - val_loss: 5.0806 - val_accuracy: 0.3600\n",
      "Epoch 19/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.0257 - accuracy: 0.9969 - val_loss: 5.2735 - val_accuracy: 0.3627\n",
      "Epoch 20/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 0.0233 - accuracy: 0.9969 - val_loss: 5.4669 - val_accuracy: 0.3633\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size=vocab_size, num_labels=11)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(train_data, validation_data=validation_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "atmospheric-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 3ms/step - loss: 5.4669 - accuracy: 0.3633\n",
      "Loss:  5.4669389724731445\n",
      "Accuracy: 36.33%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "soviet-secretariat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lyrics_classificer_1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lyrics_classificer_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-computer",
   "metadata": {},
   "source": [
    "### CLASSIFIER 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "stupid-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, 16),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_labels)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "modern-plasma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "541/541 [==============================] - 6s 9ms/step - loss: 2.3385 - accuracy: 0.2106 - val_loss: 2.2655 - val_accuracy: 0.2147\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 2.2423 - accuracy: 0.2286 - val_loss: 2.2174 - val_accuracy: 0.2257\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 2.1892 - accuracy: 0.2461 - val_loss: 2.1632 - val_accuracy: 0.2493\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 2.1293 - accuracy: 0.2703 - val_loss: 2.1089 - val_accuracy: 0.2817\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 2.0703 - accuracy: 0.2996 - val_loss: 2.0608 - val_accuracy: 0.2977\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 2.0156 - accuracy: 0.3266 - val_loss: 2.0187 - val_accuracy: 0.3213\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.9603 - accuracy: 0.3485 - val_loss: 1.9810 - val_accuracy: 0.3380\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.9102 - accuracy: 0.3710 - val_loss: 1.9481 - val_accuracy: 0.3620\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 4s 7ms/step - loss: 1.8607 - accuracy: 0.3938 - val_loss: 1.9188 - val_accuracy: 0.3737\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 5s 9ms/step - loss: 1.8137 - accuracy: 0.4126 - val_loss: 1.8926 - val_accuracy: 0.3827\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.7671 - accuracy: 0.4298 - val_loss: 1.8699 - val_accuracy: 0.3930\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.7201 - accuracy: 0.4456 - val_loss: 1.8491 - val_accuracy: 0.3993\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.6856 - accuracy: 0.4615 - val_loss: 1.8318 - val_accuracy: 0.4047\n",
      "Epoch 14/20\n",
      "541/541 [==============================] - 4s 7ms/step - loss: 1.6424 - accuracy: 0.4758 - val_loss: 1.8161 - val_accuracy: 0.4100\n",
      "Epoch 15/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.6091 - accuracy: 0.4890 - val_loss: 1.8029 - val_accuracy: 0.4123\n",
      "Epoch 16/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.5746 - accuracy: 0.4953 - val_loss: 1.7913 - val_accuracy: 0.4147\n",
      "Epoch 17/20\n",
      "541/541 [==============================] - 4s 7ms/step - loss: 1.5387 - accuracy: 0.5125 - val_loss: 1.7814 - val_accuracy: 0.4170\n",
      "Epoch 18/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.5063 - accuracy: 0.5263 - val_loss: 1.7737 - val_accuracy: 0.4190\n",
      "Epoch 19/20\n",
      "541/541 [==============================] - 4s 7ms/step - loss: 1.4788 - accuracy: 0.5307 - val_loss: 1.7666 - val_accuracy: 0.4207\n",
      "Epoch 20/20\n",
      "541/541 [==============================] - 4s 8ms/step - loss: 1.4481 - accuracy: 0.5422 - val_loss: 1.7608 - val_accuracy: 0.4230\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size=vocab_size, num_labels=11)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(train_data, validation_data=validation_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "suitable-moore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 2ms/step - loss: 1.7608 - accuracy: 0.4230\n",
      "Loss:  1.7608269453048706\n",
      "Accuracy: 42.30%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bibliographic-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lyrics_classificer_1_1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lyrics_classificer_1_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-delay",
   "metadata": {},
   "source": [
    "### CLASSIFIER 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "responsible-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_labels)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "pediatric-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "541/541 [==============================] - 13s 22ms/step - loss: 2.2856 - accuracy: 0.2143 - val_loss: 2.1039 - val_accuracy: 0.2833\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 14s 25ms/step - loss: 2.0216 - accuracy: 0.3177 - val_loss: 1.9324 - val_accuracy: 0.3600\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 13s 24ms/step - loss: 1.7406 - accuracy: 0.4265 - val_loss: 1.8579 - val_accuracy: 0.3820\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 13s 24ms/step - loss: 1.5106 - accuracy: 0.5049 - val_loss: 1.8520 - val_accuracy: 0.3903\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 12s 23ms/step - loss: 1.3182 - accuracy: 0.5697 - val_loss: 1.8784 - val_accuracy: 0.3950\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 1.1586 - accuracy: 0.6243 - val_loss: 1.9453 - val_accuracy: 0.3907\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 1.0364 - accuracy: 0.6648 - val_loss: 2.0219 - val_accuracy: 0.3917\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.9368 - accuracy: 0.6944 - val_loss: 2.1110 - val_accuracy: 0.3840\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 12s 21ms/step - loss: 0.8518 - accuracy: 0.7200 - val_loss: 2.2118 - val_accuracy: 0.3797\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 0.7962 - accuracy: 0.7348 - val_loss: 2.3280 - val_accuracy: 0.3820\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.7379 - accuracy: 0.7562 - val_loss: 2.4335 - val_accuracy: 0.3780\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 12s 23ms/step - loss: 0.6966 - accuracy: 0.7681 - val_loss: 2.5437 - val_accuracy: 0.3747\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.6551 - accuracy: 0.7796 - val_loss: 2.6533 - val_accuracy: 0.3747\n",
      "Epoch 14/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.6234 - accuracy: 0.7879 - val_loss: 2.7568 - val_accuracy: 0.3720\n",
      "Epoch 15/20\n",
      "541/541 [==============================] - 11s 21ms/step - loss: 0.5988 - accuracy: 0.7971 - val_loss: 2.8430 - val_accuracy: 0.3670\n",
      "Epoch 16/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.5727 - accuracy: 0.8038 - val_loss: 2.9398 - val_accuracy: 0.3710\n",
      "Epoch 17/20\n",
      "541/541 [==============================] - 13s 23ms/step - loss: 0.5628 - accuracy: 0.8063 - val_loss: 3.0314 - val_accuracy: 0.3767\n",
      "Epoch 18/20\n",
      "541/541 [==============================] - 12s 23ms/step - loss: 0.5316 - accuracy: 0.8170 - val_loss: 3.1107 - val_accuracy: 0.3783\n",
      "Epoch 19/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.5245 - accuracy: 0.8210 - val_loss: 3.2105 - val_accuracy: 0.3747\n",
      "Epoch 20/20\n",
      "541/541 [==============================] - 12s 22ms/step - loss: 0.5074 - accuracy: 0.8275 - val_loss: 3.2865 - val_accuracy: 0.3783\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size=vocab_size, num_labels=11)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(train_data, validation_data=validation_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "soviet-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 4ms/step - loss: 3.2865 - accuracy: 0.3783\n",
      "Loss:  3.2865211963653564\n",
      "Accuracy: 37.83%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "informative-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lyrics_classificer_1_2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lyrics_classificer_1_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-stadium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
